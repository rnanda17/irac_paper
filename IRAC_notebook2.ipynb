{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12ba6e85-5ace-4d3c-b354-e652b25eadc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 0) Imports + OpenRouter client setup\n",
    "# ============================================================\n",
    "import os, json, re\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from nltk.metrics.scores import precision, recall, f_measure\n",
    "from openai import OpenAI\n",
    "\n",
    "# open router key\n",
    "os.environ[\"OPENROUTER_API_KEY\"] = \"set your key\"\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    api_key=os.environ[\"OPENROUTER_API_KEY\"],\n",
    ")\n",
    "\n",
    "MODEL_NAME = \"google/gemini-2.5-pro\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb2e3d77-d7b1-4408-a935-2f8dbc57ae97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 1) Paths\n",
    "# ============================================================\n",
    "SYSTEM_PROMPT_PATH = Path(\"/Users/rohanpersonal/git_projs/LLM_arg_paper/question_specific_dataset/system_prompt.txt\")\n",
    "GROUND_TRUTH_PATH  = Path(\"/Users/rohanpersonal/git_projs/LLM_arg_paper/question_specific_dataset/ground_truth_irac.json\")\n",
    "\n",
    "Q_SUPERSET_CSV = {\n",
    "    \"Q1\": Path(\"/Users/rohanpersonal/git_projs/LLM_arg_paper/question_specific_dataset/Questions_superset_rows - Q1.csv\"),\n",
    "    \"Q2\": Path(\"/Users/rohanpersonal/git_projs/LLM_arg_paper/question_specific_dataset/Questions_superset_rows - Q2.csv\"),\n",
    "}\n",
    "\n",
    "OUTDIR = Path(\"/Users/rohanpersonal/git_projs/LLM_arg_paper/irac_outputs\")\n",
    "OUTDIR.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fcb5687-6173-457a-95a8-80c5a6728a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 2) Load system prompt + ground truth JSON\n",
    "# ============================================================\n",
    "SYSTEM_PROMPT = SYSTEM_PROMPT_PATH.read_text(encoding=\"utf-8\").strip()\n",
    "GROUND_TRUTH = json.loads(GROUND_TRUTH_PATH.read_text(encoding=\"utf-8\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5840db4a-2c86-4117-88b1-9bd183014df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 3) Load superset rows CSVs (NOW including Hohfeldian columns)\n",
    "#    Required columns:\n",
    "#      - Row_ID\n",
    "#      - treatytext\n",
    "#      - actorholder\n",
    "#      - action\n",
    "#      - actoraffected\n",
    "# ============================================================\n",
    "def load_superset_rows(csv_path: Path) -> pd.DataFrame:\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    required = [\"Row_ID\", \"treatytext\", \"actorholder\", \"action\", \"actoraffected\"]\n",
    "    missing = [c for c in required if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"{csv_path.name} missing columns: {missing}\")\n",
    "\n",
    "    return df[required].copy()\n",
    "\n",
    "SUPERSETS = {q: load_superset_rows(path) for q, path in Q_SUPERSET_CSV.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc60fa8b-55e6-4ae7-851b-86fdab2adfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 4) Fact patterns + C1/C2 text (fill these; later load from file)\n",
    "# ============================================================\n",
    "QUESTIONS = {\n",
    "    \"Q1\": {\n",
    "        \"fact_pattern\": \"An Australian biotechnology company conducts a private expedition to collect marine genetic resources (MGRs) in areas beyond national jurisdiction (ABNJ). The collected materials are sequenced in-house, and the resulting digital sequence information (DSI) is published in open-access repositories. The same company later relies on this DSI to develop a commercially valuable enzyme. No prior informed consent (PIC) or mutually agreed terms (MAT) were established, and no monetary or non-monetary benefit-sharing has occurred. The company argues that because the material was collected in ABNJ and only DSI is being used, rather than the physical samples, no benefit-sharing obligations apply under current international law. The competent authority of Australia has not taken regulatory steps to monitor or require disclosure of the activity. This scenario raises questions about the extent to which companies and States bear substantive obligations to ensure benefit-sharing when MGRs are accessed in ABNJ and subsequently used in commercial applications through DSI, particularly in the absence of clear jurisdictional triggers, prior agreements, or internationally coordinated oversight mechanisms.\",\n",
    "        \"C1\": \"Benefit-sharing obligations apply to the commercial utilisation of DSI derived from ABNJ MGRs in this scenario, and Australia must ensure fair and equitable benefit-sharing (including non-monetary and, where applicable, monetary benefits).\",\n",
    "        \"C2\": \"No benefit-sharing obligations apply to the commercial utilisation of DSI derived from ABNJ MGRs in this scenario, so Australia has no duty (on these facts) to ensure benefit-sharing from the company’s use of the published DSI.\",\n",
    "    },\n",
    "    \"Q2\": {\n",
    "        \"fact_pattern\": \"A university-affiliated researcher from Brazil applies for access to marine genetic resources (MGRs) located within the exclusive economic zone (EEZ) of Argentina. Both Brazil and Argentina are Parties to the Convention on Biological Diversity (CBD) and the Nagoya Protocol. The target species are found in an area inhabited by an Indigenous community with longstanding custodianship of the surrounding marine environment and traditional knowledge (TK) associated with the biological properties of the organisms. Argentina’s national competent authority grants the researcher an access permit for academic sample collection. The researcher does not engage with the Indigenous community, and neither the permitting process nor any domestic procedure includes consultation or prior informed consent (PIC) from the TK holders. No mutually agreed terms (MAT) are established, and no plans are made for benefit-sharing with the community. The samples are collected and exported to Brazil for analysis without further engagement. This fact pattern raises questions about the legal adequacy of State-issued permits that omit community-level consent when TK is clearly associated with marine genetic resources within the granting State’s jurisdiction.\",\n",
    "        \"C1\": \"Access to MGRs and associated TK located within national jurisdiction requires engagement with Indigenous Peoples and Local Communities (IPLCs), including PIC/approval and involvement and the establishment of MAT, where TK is associated with the genetic resources and where IPLCs have recognised rights.\",\n",
    "        \"C2\": \"Access to MGRs and associated TK within national jurisdiction does not require engagement with IPLCs in this scenario, because State authority over genetic resources is sufficient and IPLC PIC/approval is not legally required unless specifically mandated under domestic law.\",\n",
    "    },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0475610-5131-4884-9721-3485b90f9fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 5) Prompt builder + LLM call + JSON parse\n",
    "#    (NOW includes actor_holder / action / actor_affected per row)\n",
    "# ============================================================\n",
    "def build_user_prompt(q: str, fact_pattern: str, df_rows: pd.DataFrame, c1: str, c2: str) -> str:\n",
    "    rows = \"\\n\\n\".join(\n",
    "        \"\\n\".join([\n",
    "            str(r.Row_ID).strip(),\n",
    "            f\"actorholder: {str(r.actorholder).strip()}\",\n",
    "            f\"action: {str(r.action).strip()}\",\n",
    "            f\"actoraffected: {str(r.actoraffected).strip()}\",\n",
    "            str(r.treatytext).strip(),\n",
    "        ])\n",
    "        for _, r in df_rows.iterrows()\n",
    "    )\n",
    "\n",
    "    return f\"\"\"question_number: {q}\n",
    "\n",
    "FACT PATTERN:\n",
    "{fact_pattern}\n",
    "\n",
    "TREATY ROWS (ID + Hohfeldian mapping + exact treaty text):\n",
    "{rows}\n",
    "\n",
    "CONCLUSIONS:\n",
    "C1: {c1}\n",
    "C2: {c2}\n",
    "\"\"\".strip()\n",
    "\n",
    "def extract_json(text: str) -> dict:\n",
    "    m = re.search(r\"\\{.*\\}\", text, flags=re.DOTALL)\n",
    "    if not m:\n",
    "        raise ValueError(\"No JSON object found in model output.\")\n",
    "    return json.loads(m.group(0))\n",
    "\n",
    "def call_llm(system_prompt: str, user_prompt: str) -> dict:\n",
    "    resp = client.chat.completions.create(\n",
    "        model=MODEL_NAME,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ],\n",
    "        temperature=0,\n",
    "    extra_body={\n",
    "                \"seed\": 42,\n",
    "                \"provider\": {\"allow_fallbacks\": False},\n",
    "                \"reasoning\": {\"effort\": \"high\", \"exclude\": True},\n",
    "            },\n",
    "    )\n",
    "    return extract_json(resp.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3144fff-9d14-4dc4-95fd-14f0553ffb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 6) Validation + evaluation (rules + edges)\n",
    "# ============================================================\n",
    "def validate_pred(pred: dict):\n",
    "    assert \"question_number\" in pred\n",
    "    for k in [\"irac_C1\", \"irac_C2\"]:\n",
    "        obj = pred[k]\n",
    "        for needed in [\"issue\", \"rules_selected\", \"analysis\", \"conclusion\", \"edges_support\"]:\n",
    "            assert needed in obj\n",
    "\n",
    "        for rr in obj[\"rules_selected\"]:\n",
    "            assert \"id\" in rr and \"text\" in rr\n",
    "\n",
    "        for e in obj[\"edges_support\"]:\n",
    "            assert \"|\" in e and \" \" not in e, f\"Edge has spaces or wrong format: {e}\"\n",
    "\n",
    "def prf(gold_set: set, pred_set: set):\n",
    "    p = precision(gold_set, pred_set) or 0.0\n",
    "    r = recall(gold_set, pred_set) or 0.0\n",
    "    f1 = f_measure(gold_set, pred_set) or 0.0\n",
    "    return p, r, f1\n",
    "\n",
    "def eval_one(q: str, pred: dict, gt: dict) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for concl in [\"C1\", \"C2\"]:\n",
    "        pred_obj = pred[f\"irac_{concl}\"]\n",
    "        gt_obj = gt[q][concl]\n",
    "\n",
    "        pred_rules = {x[\"id\"].strip() for x in pred_obj[\"rules_selected\"]}\n",
    "        gold_rules = set(gt_obj[\"rules_selected\"])\n",
    "\n",
    "        pred_edges = {x.strip() for x in pred_obj[\"edges_support\"]}\n",
    "        gold_edges = set(gt_obj[\"edges_support\"])\n",
    "\n",
    "        p_r, r_r, f1_r = prf(gold_rules, pred_rules)\n",
    "        p_e, r_e, f1_e = prf(gold_edges, pred_edges)\n",
    "\n",
    "        rows += [\n",
    "            {\"question_number\": q, \"conclusion\": concl, \"metric\": \"rules_selected\",\n",
    "             \"precision\": p_r, \"recall\": r_r, \"f1\": f1_r,\n",
    "             \"gold_size\": len(gold_rules), \"pred_size\": len(pred_rules)},\n",
    "            {\"question_number\": q, \"conclusion\": concl, \"metric\": \"edges_support\",\n",
    "             \"precision\": p_e, \"recall\": r_e, \"f1\": f1_e,\n",
    "             \"gold_size\": len(gold_edges), \"pred_size\": len(pred_edges)},\n",
    "        ]\n",
    "    return pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51186bab-6485-40b9-8608-7c0ba441fcf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved outputs to: /Users/rohanpersonal/git_projs/LLM_arg_paper/irac_outputs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_number</th>\n",
       "      <th>conclusion</th>\n",
       "      <th>metric</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>gold_size</th>\n",
       "      <th>pred_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q1</td>\n",
       "      <td>C1</td>\n",
       "      <td>rules_selected</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q1</td>\n",
       "      <td>C1</td>\n",
       "      <td>edges_support</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q1</td>\n",
       "      <td>C2</td>\n",
       "      <td>rules_selected</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Q1</td>\n",
       "      <td>C2</td>\n",
       "      <td>edges_support</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q2</td>\n",
       "      <td>C1</td>\n",
       "      <td>rules_selected</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Q2</td>\n",
       "      <td>C1</td>\n",
       "      <td>edges_support</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Q2</td>\n",
       "      <td>C2</td>\n",
       "      <td>rules_selected</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Q2</td>\n",
       "      <td>C2</td>\n",
       "      <td>edges_support</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  question_number conclusion          metric  precision    recall        f1  \\\n",
       "0              Q1         C1  rules_selected   1.000000  0.666667  0.800000   \n",
       "1              Q1         C1   edges_support   0.571429  0.400000  0.470588   \n",
       "2              Q1         C2  rules_selected   0.750000  0.600000  0.666667   \n",
       "3              Q1         C2   edges_support   0.600000  0.600000  0.600000   \n",
       "4              Q2         C1  rules_selected   1.000000  0.454545  0.625000   \n",
       "5              Q2         C1   edges_support   0.750000  0.375000  0.500000   \n",
       "6              Q2         C2  rules_selected   1.000000  0.454545  0.625000   \n",
       "7              Q2         C2   edges_support   0.625000  0.454545  0.526316   \n",
       "\n",
       "   gold_size  pred_size  \n",
       "0          6          4  \n",
       "1         10          7  \n",
       "2          5          4  \n",
       "3          5          5  \n",
       "4         11          5  \n",
       "5         16          8  \n",
       "6         11          5  \n",
       "7         11          8  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 7) Run Q1 + Q2, save prediction files + metrics.csv\n",
    "# ============================================================\n",
    "all_metrics = []\n",
    "\n",
    "for q in [\"Q1\", \"Q2\"]:\n",
    "    user_prompt = build_user_prompt(\n",
    "        q=q,\n",
    "        fact_pattern=QUESTIONS[q][\"fact_pattern\"],\n",
    "        df_rows=SUPERSETS[q],\n",
    "        c1=QUESTIONS[q][\"C1\"],\n",
    "        c2=QUESTIONS[q][\"C2\"],\n",
    "    )\n",
    "\n",
    "    pred = call_llm(SYSTEM_PROMPT, user_prompt)\n",
    "    validate_pred(pred)\n",
    "\n",
    "    (OUTDIR / f\"pred_{q}.json\").write_text(json.dumps(pred, indent=2), encoding=\"utf-8\")\n",
    "    all_metrics.append(eval_one(q, pred, GROUND_TRUTH))\n",
    "\n",
    "metrics = pd.concat(all_metrics, ignore_index=True)\n",
    "metrics.to_csv(OUTDIR / \"metrics.csv\", index=False)\n",
    "\n",
    "print(\"Saved outputs to:\", OUTDIR)\n",
    "display(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389fc4fd-a888-4752-aea8-e67fcdd8082e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
